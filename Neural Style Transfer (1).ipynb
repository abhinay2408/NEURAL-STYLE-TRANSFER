{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWbX7oNqRVGp","outputId":"fe2a1206-9ef6-4b17-f205-066456be3865","executionInfo":{"status":"ok","timestamp":1746884026653,"user_tz":-330,"elapsed":4512,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os  # Add this import statement\n","\n","drive.mount('/content/drive')\n","\n","base_path = '/content/drive/MyDrive/Neural_Style_Transfer'\n","content_image_dir = f'{base_path}/content_image'\n","style_image_dir = f'{base_path}/style_image'\n","output_dir = f'{base_path}/output'\n","\n","os.makedirs(output_dir, exist_ok=True)  # Now this will work\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfDzuzxeUEtE","outputId":"fcd55f7f-13a6-4aa3-e7fb-0c659c6d88c2","executionInfo":{"status":"ok","timestamp":1746884029830,"user_tz":-330,"elapsed":3190,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}],"source":["!pip install torch torchvision pillow imageio"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fSkn6elPUypW","executionInfo":{"status":"ok","timestamp":1746884034641,"user_tz":-330,"elapsed":4816,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from torchvision import transforms, models\n","from PIL import Image\n","import numpy as np\n","import os\n","import imageio\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3BkowZPwVWW4","executionInfo":{"status":"ok","timestamp":1746884034641,"user_tz":-330,"elapsed":6,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[],"source":["def preprocess_image(image_path, transform):\n","    image = Image.open(image_path).convert('RGB')\n","    return transform(image).unsqueeze(0).to(device)\n","\n","def postprocess(tensor):\n","    image = tensor.to('cpu').clone().squeeze(0)\n","    image = image.detach().numpy().transpose(1, 2, 0)\n","    image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n","    image = np.clip(image, 0, 1)\n","    return (image * 255).astype(np.uint8)\n","\n","def gram_matrix(input_tensor):\n","    batch_size, c, h, w = input_tensor.size()\n","    features = input_tensor.view(batch_size * c, h * w)\n","    G = torch.mm(features, features.t())\n","    return G.div(batch_size * c * h * w)\n","\n","def get_features(image, model, layer_indices):\n","    features = []\n","    x = image\n","    for idx, layer in enumerate(model):\n","        x = layer(x)\n","        if idx in layer_indices:\n","            features.append(x)\n","    return features\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXji8cLoVaaF","outputId":"e256cd53-51d7-4c29-87df-d94e3588c976","executionInfo":{"status":"ok","timestamp":1746884037511,"user_tz":-330,"elapsed":2874,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["# Configuration\n","content_path = f\"{content_image_dir}/content1.jpeg\"\n","style_path = f\"{style_image_dir}/style3.jpeg\"\n","iterations = 1000\n","content_weight = 1e2\n","style_weight = 1e7\n","lr = 0.05\n","\n","# Transform\n","transform = transforms.Compose([\n","    transforms.Resize(512),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load images\n","content = preprocess_image(content_path, transform)\n","style = preprocess_image(style_path, transform)\n","\n","# Load VGG19\n","vgg = models.vgg19(pretrained=True).features.to(device).eval()\n","for param in vgg.parameters():\n","    param.requires_grad_(False)\n","\n","content_layers = [22]\n","style_layers = [1, 6, 11, 20, 29] # Keep these for now, but consider experimenting\n","\n","content_features = get_features(content, vgg, content_layers)\n","style_features = get_features(style, vgg, style_layers)\n","style_grams = [gram_matrix(f) for f in style_features]\n","\n","# Initial image\n","target = content.clone().requires_grad_(True)\n","optimizer = optim.LBFGS([target], lr=lr)\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWBgaCXnVdO3","outputId":"ffbc4cde-d32c-46cd-cdcc-eb16d48029cd","executionInfo":{"status":"ok","timestamp":1746884286723,"user_tz":-330,"elapsed":249221,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting optimization...\n","Iteration 0 | Content Loss: 0.00, Style Loss: 1333.08\n","Iteration 100 | Content Loss: 93.34, Style Loss: 903.43\n","Iteration 200 | Content Loss: 105.59, Style Loss: 682.08\n","Iteration 300 | Content Loss: 102.70, Style Loss: 535.90\n","Iteration 400 | Content Loss: 100.42, Style Loss: 421.82\n","Iteration 500 | Content Loss: 97.83, Style Loss: 321.12\n","Iteration 600 | Content Loss: 94.30, Style Loss: 247.65\n","Iteration 700 | Content Loss: 90.68, Style Loss: 200.95\n","Iteration 800 | Content Loss: 87.31, Style Loss: 171.93\n","Iteration 900 | Content Loss: 84.29, Style Loss: 153.37\n","Iteration 1000 | Content Loss: 81.90, Style Loss: 140.97\n","Optimization complete!\n"]}],"source":["intermediate_images = []\n","iteration = [0]\n","\n","print('Starting optimization...')\n","\n","while iteration[0] <= iterations:\n","    def closure():\n","        optimizer.zero_grad()\n","        target_content = get_features(target, vgg, content_layers)\n","        target_style = get_features(target, vgg, style_layers)\n","        target_grams = [gram_matrix(layer) for layer in target_style]\n","\n","        content_loss = content_weight * torch.mean((target_content[0] - content_features[0])**2)\n","        style_loss = 0\n","        for tg, sg in zip(target_grams, style_grams):\n","            style_loss += torch.mean((tg - sg)**2)\n","        style_loss *= style_weight\n","        total_loss = content_loss + style_loss\n","        total_loss.backward()\n","\n","        if iteration[0] % 100 == 0:\n","            print(f\"Iteration {iteration[0]} | Content Loss: {content_loss.item():.2f}, Style Loss: {style_loss.item():.2f}\")\n","            with torch.no_grad():\n","                img = postprocess(target)\n","                intermediate_images.append(img)\n","                Image.fromarray(img).save(os.path.join(output_dir, f'iter_{iteration[0]}.png'))\n","        iteration[0] += 1\n","        return total_loss\n","    optimizer.step(closure)\n","\n","print('Optimization complete!')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKrsgA9FVlHs","outputId":"c08e1db4-2ae8-47e8-f736-31a98bc500d7","executionInfo":{"status":"ok","timestamp":1746884293459,"user_tz":-330,"elapsed":6742,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved final outputs to: /content/drive/MyDrive/Neural_Style_Transfer/output\n"]}],"source":["# Save final image\n","final_image = postprocess(target)\n","Image.fromarray(final_image).save(os.path.join(output_dir, 'final.jpg'))\n","\n","# Save progress GIF\n","if intermediate_images:\n","    imageio.mimsave(os.path.join(output_dir, 'progress.gif'),\n","                    [Image.fromarray(img) for img in intermediate_images], duration=500, loop=0)\n","print(\"Saved final outputs to:\", output_dir)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"eFAimfscY0Li","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1cgOF4zK5cxnOrvUZH-jB7WoBfpgn-tjn"},"executionInfo":{"status":"ok","timestamp":1746884293459,"user_tz":-330,"elapsed":44,"user":{"displayName":"Abhinay Rendla","userId":"12079157040803782186"}},"outputId":"cf9d0b59-d735-4e00-d440-5a6de1443b74"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#print the above output image in this notebook\n","\n","from IPython.display import Image as IPythonImage\n","\n","# Display the final generated image\n","final_image_path = os.path.join(output_dir, 'final.jpg')\n","display(IPythonImage(filename=final_image_path))\n","\n","# Display the progress GIF\n","if intermediate_images:\n","    progress_gif_path = os.path.join(output_dir, 'progress.gif')\n","    display(IPythonImage(filename=progress_gif_path))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}